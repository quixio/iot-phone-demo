# Import the Quix Streams modules for interacting with Kafka:
from quixstreams import Application
# (see https://quix.io/docs/quix-streams/v2-0-latest/api-reference/quixstreams.html for more details)

# Import additional modules as needed
import pandas as pd
import json
import random
import time
import os

# import the dotenv module to load environment variables from a file
from dotenv import load_dotenv

load_dotenv(override=False)

# Create an Application.
app = Application.Quix()

# Define the topic using the "output" environment variable
topic_name = os.getenv("output", "")
if topic_name == "":
    raise ValueError("The 'output' environment variable is required. This is the output topic that data will be published to.")

topic = app.topic(topic_name)

# Get the directory of the current script
script_dir = os.path.dirname(os.path.realpath(__file__))
# Construct the path to the CSV file
csv_file_path = os.path.join(script_dir, "demo-data.csv")


def main():
    """
    Read data from the CSV file and publish it to Kafka
    """

    # Create a pre-configured Producer object.
    # Producer is already setup to use Quix brokers.
    # It will also ensure that the topics exist before producing to them if
    # Application.Quix is initiliazed with "auto_create_topics=True".
    producer = app.get_producer()

    with producer:
        message_key = f"CSV_DATA_{str(random.randint(1, 100)).zfill(3)}"
        
        df = pd.read_csv(csv_file_path)
        print("File loaded.")
        
        # Get the column headers as a list
        headers = df.columns.tolist()
            
        for _, row in df.iterrows():
            # Create a dictionary that includes both column headers and row values
            # Replace NaN values with None (which will become 'null' in JSON)
            row_data = {header: (None if pd.isna(row[header]) else row[header]) for header in headers}

            # We are simulating data so removing original timestamp.
            # We will use Kafka autogenerated timestamp instead.
            del row_data["Timestamp"]
            
            # Serialize row_data to a JSON string
            json_data = json.dumps(row_data)

            # publish the data to the topic
            producer.produce(
                topic=topic.name,
                key=message_key,
                value=json_data
            )

            print(row_data)
            
            time.sleep(0.2)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Exiting.")